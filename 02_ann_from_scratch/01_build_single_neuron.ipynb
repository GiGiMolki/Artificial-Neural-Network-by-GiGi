{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Build a Single Neuron from Scratch\n",
    "\n",
    "In this notebook, we implement a single artificial neuron (perceptron) from scratch using NumPy. We'll explore how a neuron processes input, applies weights and bias, uses an activation function, and produces an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Neuron Equation\n",
    "\n",
    "A single neuron computes the following:\n",
    "\n",
    "\\[\n",
    "z = w \\cdot x + b \\quad \\text{and} \\quad a = \\phi(z)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( w \\): weights\n",
    "- \\( x \\): inputs\n",
    "- \\( b \\): bias\n",
    "- \\( z \\): linear combination\n",
    "- \\( \\phi \\): activation function (e.g., sigmoid, ReLU)\n",
    "- \\( a \\): activated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 1: Define Activation Functions\n",
    "\n",
    "We use sigmoid to squash the output between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß± Step 2: Create a Single Neuron Class\n",
    "\n",
    "We define a class that simulates a basic neuron with:\n",
    "- `forward()` for computing output\n",
    "- `train()` for updating weights using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron:\n",
    "    def __init__(self, input_dim, lr=0.1):\n",
    "        self.weights = np.random.randn(input_dim)\n",
    "        self.bias = 0.0\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        return sigmoid(z)\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        for _ in range(epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                z = np.dot(self.weights, xi) + self.bias\n",
    "                a = sigmoid(z)\n",
    "                error = a - target\n",
    "                grad = error * sigmoid_derivative(z)\n",
    "                self.weights -= self.lr * grad * xi\n",
    "                self.bias -= self.lr * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 3: Train the Neuron (Example: OR Gate)\n",
    "\n",
    "Train the neuron on an OR gate where:\n",
    "- Inputs: (0,0), (0,1), (1,0), (1,1)\n",
    "- Output: 0, 1, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Output: 0.1884\n",
      "Input: [0 1], Output: 0.8915\n",
      "Input: [1 0], Output: 0.8832\n",
      "Input: [1 1], Output: 0.9963\n"
     ]
    }
   ],
   "source": [
    "# Input features and labels\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([0, 1, 1, 1])\n",
    "\n",
    "neuron = SingleNeuron(input_dim=2, lr=0.1)\n",
    "neuron.train(X, y, epochs=1000)\n",
    "\n",
    "# Test predictions\n",
    "for x in X:\n",
    "    output = neuron.forward(x)\n",
    "    print(f\"Input: {x}, Output: {output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- A neuron computes a weighted sum plus bias.\n",
    "- The activation function introduces non-linearity.\n",
    "- Training uses gradient descent to update weights.\n",
    "- We trained a single neuron to simulate the OR logic gate.\n",
    "\n",
    "Next: We'll explore multi-neuron single-layer networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
