{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Neuron Basics ‚Äì Understanding the Building Block of ANN\n",
    "\n",
    "In this notebook, we will build the intuition and code for a **single artificial neuron**.\n",
    "\n",
    "We will cover:\n",
    "- What a neuron does mathematically\n",
    "- Weight, bias, and activation function\n",
    "- Implementing a simple neuron from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ The Neuron Equation\n",
    "\n",
    "A neuron takes multiple inputs and produces an output using this formula:\n",
    "\n",
    "\\[\n",
    "z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b = \\mathbf{w} \\cdot \\mathbf{x} + b\n",
    "\\]\n",
    "\n",
    "Then it applies an activation function:\n",
    "\n",
    "\\[\n",
    "a = \\phi(z)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{x} \\): input vector\n",
    "- \\( \\mathbf{w} \\): weight vector\n",
    "- \\( b \\): bias\n",
    "- \\( \\phi \\): activation function (e.g., sigmoid, ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Neuron Implementation in Python\n",
    "\n",
    "We'll build a class that:\n",
    "- Accepts weights and bias\n",
    "- Computes \\( z = \\mathbf{w} \\cdot \\mathbf{x} + b \\)\n",
    "- Applies the chosen activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias, activation='sigmoid'):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activation = relu\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z = np.dot(self.weights, inputs) + self.bias\n",
    "        a = self.activation(z)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Example: Simulate a Neuron\n",
    "\n",
    "Let's give 3 inputs to a neuron with 3 weights and a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron output: 0.7841\n"
     ]
    }
   ],
   "source": [
    "inputs = [0.5, 1.2, -0.3]\n",
    "weights = [0.4, 0.7, -0.5]\n",
    "bias = 0.1\n",
    "\n",
    "neuron = Neuron(weights=weights, bias=bias, activation='sigmoid')\n",
    "output = neuron.forward(inputs)\n",
    "\n",
    "print(f\"Neuron output: {output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- A neuron performs a weighted sum and adds a bias.\n",
    "- It then passes the result through a non-linear **activation function**.\n",
    "- This simple unit is the core of every neural network.\n",
    "\n",
    "In the next notebook, we'll explore **activation functions** in more depth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
